{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT Based NER",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvQ_RWMLbbJR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e3905e38-8bf9-4e67-9b62-bcf32470c7ce"
      },
      "source": [
        "!git clone https://github.com/kamalkraj/BERT-NER.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'BERT-NER'...\n",
            "remote: Enumerating objects: 246, done.\u001b[K\n",
            "remote: Total 246 (delta 0), reused 0 (delta 0), pack-reused 246\u001b[K\n",
            "Receiving objects: 100% (246/246), 1.67 MiB | 20.37 MiB/s, done.\n",
            "Resolving deltas: 100% (125/125), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWwo-o7sfIvN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e7d2cc9-eac8-45d1-c75d-0a7cb99b76e6"
      },
      "source": [
        "cd BERT-NER/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/BERT-NER\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9sflbGTrlaJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "cac62f89-a832-4a5b-af6c-494789a7afc0"
      },
      "source": [
        "!wget https://www.dropbox.com/s/x0uhua7h4bcfvsf/out_ner-20200923T154226Z-001.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-23 15:56:51--  https://www.dropbox.com/s/x0uhua7h4bcfvsf/out_ner-20200923T154226Z-001.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.1, 2620:100:6016:1::a27d:101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/x0uhua7h4bcfvsf/out_ner-20200923T154226Z-001.zip [following]\n",
            "--2020-09-23 15:56:51--  https://www.dropbox.com/s/raw/x0uhua7h4bcfvsf/out_ner-20200923T154226Z-001.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc53d410b6310d9f03a50169edf3.dl.dropboxusercontent.com/cd/0/inline/A___zzRB3lc2sXfdCHufaPx-q9-55Km-91sjOGPpX4PHO24YL4qYlMD4PLkSV64kaDOj-GupCFyJSlYT685z0QSxWi7dqRWRKrR40issdMhdThNJOZfkyoMhgJn0IO3rT08/file# [following]\n",
            "--2020-09-23 15:56:51--  https://uc53d410b6310d9f03a50169edf3.dl.dropboxusercontent.com/cd/0/inline/A___zzRB3lc2sXfdCHufaPx-q9-55Km-91sjOGPpX4PHO24YL4qYlMD4PLkSV64kaDOj-GupCFyJSlYT685z0QSxWi7dqRWRKrR40issdMhdThNJOZfkyoMhgJn0IO3rT08/file\n",
            "Resolving uc53d410b6310d9f03a50169edf3.dl.dropboxusercontent.com (uc53d410b6310d9f03a50169edf3.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n",
            "Connecting to uc53d410b6310d9f03a50169edf3.dl.dropboxusercontent.com (uc53d410b6310d9f03a50169edf3.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/A_-09kZe7TmSWJY66uQh5a9JTVoyie1lo2BwDSPqSeLhKjE7-yiklOAFv27EMCnunx0fM2XZEnxVph3akTHTNpQbp6Qz3NLOm7VN4Y32xciYH1cUGJ5pdlWtX0moHEOiCBvjOXmfjr_qjUrsV4VFsVFquRvFBoui-yhitUXy2uqYz-ea-TTfa8UDkzVrACTcorirnzIbVLt7Nvu7lroDOhtjLfqM4Z-VZauZ41awGje6QZzdzv6V088OgyG9BIkuhSsrPgxDyrUUHlQoNH06GII0otaOkY1WdKAozSlJU8oHMwg_LF48wIQjyW2goUvI--Bd-IRfOjHy2apoodXct_Rhz3n13s6Ki8tu0cQ-ebaVew/file [following]\n",
            "--2020-09-23 15:56:52--  https://uc53d410b6310d9f03a50169edf3.dl.dropboxusercontent.com/cd/0/inline2/A_-09kZe7TmSWJY66uQh5a9JTVoyie1lo2BwDSPqSeLhKjE7-yiklOAFv27EMCnunx0fM2XZEnxVph3akTHTNpQbp6Qz3NLOm7VN4Y32xciYH1cUGJ5pdlWtX0moHEOiCBvjOXmfjr_qjUrsV4VFsVFquRvFBoui-yhitUXy2uqYz-ea-TTfa8UDkzVrACTcorirnzIbVLt7Nvu7lroDOhtjLfqM4Z-VZauZ41awGje6QZzdzv6V088OgyG9BIkuhSsrPgxDyrUUHlQoNH06GII0otaOkY1WdKAozSlJU8oHMwg_LF48wIQjyW2goUvI--Bd-IRfOjHy2apoodXct_Rhz3n13s6Ki8tu0cQ-ebaVew/file\n",
            "Reusing existing connection to uc53d410b6310d9f03a50169edf3.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 402280934 (384M) [application/zip]\n",
            "Saving to: ‘out_ner-20200923T154226Z-001.zip’\n",
            "\n",
            "out_ner-20200923T15 100%[===================>] 383.64M  92.3MB/s    in 4.2s    \n",
            "\n",
            "2020-09-23 15:56:56 (92.1 MB/s) - ‘out_ner-20200923T154226Z-001.zip’ saved [402280934/402280934]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHz4c6kUrqQ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "2f4e7b3c-c477-4faa-dc3d-d98acfdbefae"
      },
      "source": [
        "!unzip out_ner-20200923T154226Z-001.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  out_ner-20200923T154226Z-001.zip\n",
            "  inflating: out_ner/eval_results.txt  \n",
            "  inflating: out_ner/model_config.json  \n",
            "  inflating: out_ner/vocab.txt       \n",
            "  inflating: out_ner/added_tokens.json  \n",
            "  inflating: out_ner/special_tokens_map.json  \n",
            "  inflating: out_ner/config.json     \n",
            "  inflating: out_ner/tokenizer_config.json  \n",
            "  inflating: out_ner/pytorch_model.bin  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDphixDqmufw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4430a9ac-4b0b-4632-f458-45166a719af3"
      },
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-transformers==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 4.7MB/s \n",
            "\u001b[?25hCollecting torch==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/57/d5cceb0799c06733eefce80c395459f28970ebb9e896846ce96ab579a3f1/torch-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (748.8MB)\n",
            "\u001b[K     |████████████████████████████████| 748.9MB 15kB/s \n",
            "\u001b[?25hCollecting seqeval==0.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/dc/b6/6e58b54c0fa343f9c24969cb887f3e76c13d16dded640cc620a914f27dc4/seqeval-0.0.5-py3-none-any.whl\n",
            "Collecting tqdm==4.31.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.7MB/s \n",
            "\u001b[?25hCollecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 53.9MB/s \n",
            "\u001b[?25hCollecting Flask==1.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/93/628509b8d5dc749656a9641f4caf13540e2cdec85276964ff8f43bbb1d3b/Flask-1.1.1-py2.py3-none-any.whl (94kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 14.1MB/s \n",
            "\u001b[?25hCollecting Flask-Cors==3.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.2.0->-r requirements.txt (line 1)) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 47.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.2.0->-r requirements.txt (line 1)) (1.14.63)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.2.0->-r requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.2.0->-r requirements.txt (line 1)) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 49.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.4.5->-r requirements.txt (line 8)) (1.15.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask==1.1.1->-r requirements.txt (line 10)) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask==1.1.1->-r requirements.txt (line 10)) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask==1.1.1->-r requirements.txt (line 10)) (2.11.2)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask==1.1.1->-r requirements.txt (line 10)) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.2.0->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.2.0->-r requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.2.0->-r requirements.txt (line 1)) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.2.0->-r requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers==1.2.0->-r requirements.txt (line 1)) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers==1.2.0->-r requirements.txt (line 1)) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.63 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers==1.2.0->-r requirements.txt (line 1)) (1.17.63)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers==1.2.0->-r requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask==1.1.1->-r requirements.txt (line 10)) (1.1.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.63->boto3->pytorch-transformers==1.2.0->-r requirements.txt (line 1)) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.63->boto3->pytorch-transformers==1.2.0->-r requirements.txt (line 1)) (2.8.1)\n",
            "Building wheels for collected packages: nltk, sacremoses\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449907 sha256=20696d00e1c0b4e24cbc827e54085ec507664a7f2083e35ef95dc6ffee50825e\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=4f5062607d77a49b661923124e6c6dc260c5448f03c0af9cb801d94f84bd4e27\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built nltk sacremoses\n",
            "\u001b[31mERROR: torchvision 0.7.0+cu101 has requirement torch==1.6.0, but you'll have torch 1.2.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.31.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement tqdm>=4.36.1, but you'll have tqdm 4.31.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sentencepiece, tqdm, torch, sacremoses, pytorch-transformers, seqeval, nltk, Flask, Flask-Cors\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Found existing installation: Flask 1.1.2\n",
            "    Uninstalling Flask-1.1.2:\n",
            "      Successfully uninstalled Flask-1.1.2\n",
            "Successfully installed Flask-1.1.1 Flask-Cors-3.0.8 nltk-3.4.5 pytorch-transformers-1.2.0 sacremoses-0.0.43 sentencepiece-0.1.91 seqeval-0.0.5 torch-1.2.0 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBcE-D7siTnm",
        "colab_type": "text"
      },
      "source": [
        "### **Fine-Tuning:**\n",
        "> For finetuning or training the **bert-base** model run the 'run_ner.py' as command given below.\n",
        "\n",
        "> In below command we have to pass different arguments:\n",
        "   \n",
        "*   '--data_dir' argument required to collect dataset. Pass 'data/' as argument which we can see as directory inside 'BERT-NER' folder for the previous comment and command for 'BERT-NER files' .\n",
        "*   '--bert_model' used to download **pretrained bert base** model of Hugging Face transformers. There are different model-names as suggested by hugging face for argument, here we select 'bert-base-cased'.\n",
        "*   '--task_name' argument used for task to perform. Enter 'ner' as we will train the model for Named Entity Recogintion(NER).\n",
        "*   '--output_dir' argument is for where to store fine-tuned model. We give name 'out_base' for directory  where fine-tuned model stored.\n",
        "*   Other arguments like '--max_seq_length', '--num_train_epochs' and '--warmup_proportion', just give values as suggested in repository.\n",
        "*   For training pass argument '--do_train' and after that evaluating for results pass argument '--do_eval'.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLFE2bp9f7Vv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python run_ner.py --data_dir=data/ --bert_model=bert-base-cased --task_name=ner --output_dir=out_ner --max_seq_length=128 --do_train --num_train_epochs 3 --do_eval --warmup_proportion=0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4qc2xmMA_z8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "da2d21f1-a9f5-4712-f336-abcdf8fafb94"
      },
      "source": [
        "%%writefile bert.py\n",
        "\"\"\"BERT NER Inference.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from nltk import word_tokenize\n",
        "from pytorch_transformers import (BertConfig, BertForTokenClassification,\n",
        "                                  BertTokenizer)\n",
        "\n",
        "\n",
        "class BertNer(BertForTokenClassification):\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, valid_ids=None):\n",
        "        sequence_output = self.bert(input_ids, token_type_ids, attention_mask, head_mask=None)[0]\n",
        "        batch_size,max_len,feat_dim = sequence_output.shape\n",
        "        valid_output = torch.zeros(batch_size,max_len,feat_dim,dtype=torch.float32,device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        for i in range(batch_size):\n",
        "            jj = -1\n",
        "            for j in range(max_len):\n",
        "                    if valid_ids[i][j].item() == 1:\n",
        "                        jj += 1\n",
        "                        valid_output[i][jj] = sequence_output[i][j]\n",
        "        sequence_output = self.dropout(valid_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "        return logits\n",
        "\n",
        "class Ner:\n",
        "\n",
        "    def __init__(self,model_dir: str):\n",
        "        self.model , self.tokenizer, self.model_config = self.load_model(model_dir)\n",
        "        self.label_map = self.model_config[\"label_map\"]\n",
        "        self.max_seq_length = self.model_config[\"max_seq_length\"]\n",
        "        self.label_map = {int(k):v for k,v in self.label_map.items()}\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def load_model(self, model_dir: str, model_config: str = \"model_config.json\"):\n",
        "        model_config = os.path.join(model_dir,model_config)\n",
        "        model_config = json.load(open(model_config))\n",
        "        model = BertNer.from_pretrained(model_dir)\n",
        "        tokenizer = BertTokenizer.from_pretrained(model_dir, do_lower_case=model_config[\"do_lower\"])\n",
        "        return model, tokenizer, model_config\n",
        "\n",
        "    def tokenize(self, text: str):\n",
        "        \"\"\" tokenize input\"\"\"\n",
        "        words = word_tokenize(text)\n",
        "        tokens = []\n",
        "        valid_positions = []\n",
        "        for i,word in enumerate(words):\n",
        "            token = self.tokenizer.tokenize(word)\n",
        "            tokens.extend(token)\n",
        "            for i in range(len(token)):\n",
        "                if i == 0:\n",
        "                    valid_positions.append(1)\n",
        "                else:\n",
        "                    valid_positions.append(0)\n",
        "        return tokens, valid_positions\n",
        "\n",
        "    def preprocess(self, text: str):\n",
        "        \"\"\" preprocess \"\"\"\n",
        "        tokens, valid_positions = self.tokenize(text)\n",
        "        ## insert \"[CLS]\"\n",
        "        tokens.insert(0,\"[CLS]\")\n",
        "        valid_positions.insert(0,1)\n",
        "        ## insert \"[SEP]\"\n",
        "        tokens.append(\"[SEP]\")\n",
        "        valid_positions.append(1)\n",
        "        segment_ids = []\n",
        "        for i in range(len(tokens)):\n",
        "            segment_ids.append(0)\n",
        "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        input_mask = [1] * len(input_ids)\n",
        "        while len(input_ids) < self.max_seq_length:\n",
        "            input_ids.append(0)\n",
        "            input_mask.append(0)\n",
        "            segment_ids.append(0)\n",
        "            valid_positions.append(0)\n",
        "        return input_ids,input_mask,segment_ids,valid_positions\n",
        "\n",
        "    def predict(self, text: str):\n",
        "        input_ids,input_mask,segment_ids,valid_ids = self.preprocess(text)\n",
        "        input_ids = torch.tensor([input_ids],dtype=torch.long,device=self.device)\n",
        "        input_mask = torch.tensor([input_mask],dtype=torch.long,device=self.device)\n",
        "        segment_ids = torch.tensor([segment_ids],dtype=torch.long,device=self.device)\n",
        "        valid_ids = torch.tensor([valid_ids],dtype=torch.long,device=self.device)\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(input_ids, segment_ids, input_mask,valid_ids)\n",
        "        logits = F.softmax(logits,dim=2)\n",
        "        logits_label = torch.argmax(logits,dim=2)\n",
        "        logits_label = logits_label.detach().cpu().numpy().tolist()[0]\n",
        "\n",
        "        logits_confidence = [values[label].item() for values,label in zip(logits[0],logits_label)]\n",
        "\n",
        "        logits = []\n",
        "        pos = 0\n",
        "        for index,mask in enumerate(valid_ids[0]):\n",
        "            if index == 0:\n",
        "                continue\n",
        "            if mask == 1:\n",
        "                logits.append((logits_label[index-pos],logits_confidence[index-pos]))\n",
        "            else:\n",
        "                pos += 1\n",
        "        logits.pop()\n",
        "\n",
        "        labels = [(self.label_map[label],confidence) for label,confidence in logits]\n",
        "        words = word_tokenize(text)\n",
        "        assert len(labels) == len(words)\n",
        "\n",
        "        Person = []\n",
        "        Location = []\n",
        "        Organization = []\n",
        "        Miscelleneous = []\n",
        "\n",
        "        for word, (label, confidence) in zip(words, labels):\n",
        "            if label==\"B-PER\" or label==\"I-PER\":\n",
        "                Person.append(word)\n",
        "            elif label==\"B-LOC\" or label==\"I-LOC\":\n",
        "                Location.append(word)\n",
        "            elif label==\"B-ORG\" or label==\"I-ORG\":\n",
        "                Organization.append(word)\n",
        "            elif label==\"B-MISC\" or label==\"I-MISC\":\n",
        "                Miscelleneous.append(word)\n",
        "            else:\n",
        "                output = None\n",
        "\n",
        "        output = []\n",
        "        for word, (label, confidence) in zip(words, labels):      \n",
        "            if label == \"B-PER\":\n",
        "                output.append(' '.join(Person) + \": Person\")\n",
        "            if label==\"B-LOC\":\n",
        "                output.append(' '.join(Location) + \": Location\")\n",
        "            if label==\"B-MISC\":\n",
        "                output.append(' '.join(Miscelleneous) + \": Miscelleneous Entity\")\n",
        "            if label==\"B-ORG\":\n",
        "                output.append(' '.join(Organization) + \": Organization\")\n",
        "                \n",
        "        return output\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting bert.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJpTRPE2rOQo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d9b1ea73-14c0-4bfa-802b-5382d54dfdfa"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRseIPZ1rcEG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "e3996a87-67a0-4bbb-ced0-b3234bc0584d"
      },
      "source": [
        "from bert import Ner\n",
        "model = Ner(\"out_ner/\")\n",
        "\n",
        "text= \"Russia made a deepfake about the US president Donald Trump working as a reporter after losing elections.\"\n",
        "print(\"Text to predict Entity:\", text)\n",
        "\n",
        "output = model.predict(text)\n",
        "for prediction in output:\n",
        "    print(prediction)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text to predict Entity: Russia made a deepfake about the US president Donald Trump working as a reporter after losing elections.\n",
            "Russia US: Location\n",
            "Russia US: Location\n",
            "Donald Trump: Person\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2xP33O3tER4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}